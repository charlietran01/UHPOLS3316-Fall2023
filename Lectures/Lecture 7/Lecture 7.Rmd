---
title: 'Lecture : Probabilities continued and Frequency Distributions (POLS 3316 -
  Statistics for Political Scientists)'
author: "Tom Hanna"
output:
  html_document:
    df_print: paged
---

### Upcoming
 
+ Problem Set 3 - uses data you pick
+ Labs about your data: Loading it, Applying today's functions to it
+ First data I get will be the example if possible: 1 outside data set and 1 built-in R dataset
+ I'll need to know where to get the data and what variables you are interested in  

### Recap

+ P(A) = $\frac{Favorable Outcomes}{All Possible Outcomes}$ (where "favorable" means outcomes where A occurs)
+ P(A) is the proportion we expect if we repeat the process a large number of times
+ Proportion of *elements* or *subsets* to the *sample space*
+ Important terms: *union*, *intersection*, *mutually exclusive*, *empty set*, *complement*, *event*
+ Probability rules: 0 to 1. P(Something) = 1. 


### Non-mutually exclusive sets

+ The probability for the union of mutually exclusive sets was the sum of their probabilities: P(A $\cup$ B) =  P(A) + P(B)
+ With non-mutually exclusive sets this results in double counting. It could lead to P > 1

Example:

+ S = {A,B,C,D,E,F,G,H} (8 possibilities)
+ B = {A,B,C,D,E} (5 favorable)
+ A = {D,E,F,G,H} (5 favorable)
+ P(A) = 5/8  and P(B) = 5/8
+ P(A $\cup$ B) = 10/8 violates the rule P = 0 to 1
+ {D,E} got counted twice
+ {D,E} is A $\cap$ B
+ So for non-exclusive pairs, our formula is:

**P(A $\cup$ B) = P(A) + P(B) - P(A $\cap$ B)**

*Note that this actually works for mutually exclusive sets to because for mutually exclusive sets, the intersection is the empty set and has P = 0*

$$\\[4in]$$

### Independence

+ For now, problems will assume independence unless explicitly specified * 
+ Non-independent (conditional) events are covered by Bayes Rule
+ You don't have to worry about Bayes Rule for now except to understand that it exists and applies to non-independent events
+ Independent events are unrelated
+ If learning the probability of one event (A) doesn't affect the probability of the other event (B), they are independent
+ Example: If I draw a number from a hat, then flip a coin the outcome of the draw doesn't affect the outcome of the coin flip.
+ P(A $\cap$ B) = P(A)P(B) for independent events
+ The probability rules for non-independent events, called *conditional probability*, are different



###
* Question: What if on a short answer test question, I ask: "Event A and Event B are not independent. How would you determine the conditional probability of event A given event B?" What would you answer?

$$\\[4in]$$

Answer: I would apply Bayes Rule. **OR** I would construct a sample space and determine the probabilities with set theory. (This is what Bayes actually does.)  

$$\\[4in]$$



### Data Assumptions: Random, Independent, and Identically Distributed

+ Randomness and independence matter as assumptions about data
+ Specifically, these are assumptions about the *Data Generating Process* of DGP
+ Purpose: analyzing data for causal inference (to begin to make statements about cause and effect - inferring causes)
+ The source of the data matters - the DGP matters
+ Previously stated: Data comes from a random world
+ So the DGP is random
+ Events in the data are *independent and identically distributed* - the IID assumption
+ Independence is statistical independence - the outcome of one event does not affect our belief about the probability of another event
+ Identically distributed: drawn from the same *probability distribution* 

$$\\[4in]$$

### Introduction to distributions

+ R has functions for at least 20 distributions
+ The most important is the *normal distribution*
+ This is because of the *central limit theorem*
+ We will look at these in the most detail: *normal*, *binomial*, *uniform*, *poisson* 

$$\\[4in]$$

#### Uniform distribution

**All outcomes are equally likely**

```{r}
rand.unif <- runif(100000, min = 0, max = 10)
hist(rand.unif, breaks = 20, freq = TRUE, main = "uniform distribution of 100,000 random draws", xlab = 'x', col = "red")
```
$$\\[4in]$$

#### Normal Distribution

+ *symmetrical around its mean with most values near the central peak*
+ width is a function of the *standard deviation*
+ Other names: *Gaussian distribution*, *bell curve*

```{r}
rand.norm<- rnorm(100000)
hist(rand.norm, breaks = 200, freq = TRUE, main = "normal distribution, sd = 1, 100,000 random draws", xlab = 'x', col = "red")

```

$$\\[4in]$$

#### Binomial Distribution

+ binary 
+ success/failure
+ yes/no

```{r}

rand.binom<- rbinom(100000,1,.5)
hist(rand.binom, breaks = 200, freq = TRUE, main = "binomial distribution, p = .5, 1 trial, 100,000 draws", xlab = 'x', col = "red")

```
$$\\[4in]$$

## Preview of the Central Limit Theorem

What happens if we do the same thing above but do it 1,000 times and plot the counts?

```{r, figures-side, fig.show="hold", out.width="50%"}

rand.binom3<- rbinom(100000,100,.5)
hist(rand.binom3, breaks = 200, freq = TRUE, main = "Histogram of binomial distribution, p = .5, 1 trial, 100,000 draws", xlab = 'x', col = "red")

rand.binom4<- rbinom(100000,1000,.5)
hist(rand.binom4, breaks = 200, freq = TRUE, main = "Histogram of binomial distribution, p = .5, 1000 trials, 100,000 draws", xlab = 'x', col = "red")

```
### Poisson distribution

+ *Count* of number of events in a fixed time/space
+ Known constant mean rate
+ Independent of time since last event

```{r}

rand.poiss<- rpois(100000,1)
hist(rand.poiss, breaks = 200, freq = TRUE, main = "poisson distribution, lambda = 1, 100,000 draws", xlab = 'x', col = "red")

```